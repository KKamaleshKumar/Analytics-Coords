{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd06dce376d6802de2db05f38814d622594779557761cf1ad656f198897303a7367",
   "display_name": "Python 3.8.10 64-bit ('torch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "6dce376d6802de2db05f38814d622594779557761cf1ad656f198897303a7367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import *\n",
    "from model import *\n",
    "from train import *"
   ]
  },
  {
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Ensuring unbiased split of dataset by distributing the classes equally."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = %pwd\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "main_df = main_df.sample(n=main_df.shape[0])\n",
    "main_df = main_df[[\"question_text\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_class = main_df.loc[main_df.target == 0, :]\n",
    "l_class = main_df.loc[main_df.target == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_o = o_class.iloc[:10000, :]\n",
    "test_l = l_class.iloc[:10000, :]\n",
    "\n",
    "valid_o = o_class.iloc[10000:20000, :]\n",
    "valid_l = l_class.iloc[10000:20000, :]\n",
    "\n",
    "train_o = o_class.iloc[20000:, :]\n",
    "train_l = l_class.iloc[20000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_o, train_l], axis=0)\n",
    "valid = pd.concat([valid_o, valid_l], axis=0)\n",
    "test = pd.concat([test_o, test_l], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir inputs"
   ]
  },
  {
   "source": [
    "### Saving the preprocessed dataset \n",
    "To avoid multiple computations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(PATH, \"inputs/train.csv\"), index=False)\n",
    "test.to_csv(os.path.join(PATH, \"inputs/test.csv\"), index=False)\n",
    "valid.to_csv(os.path.join(PATH, \"inputs/valid.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the useless dataframes to free up memory\n",
    "del main_df, train, test, valid, train_l, train_o, test_l, test_o, valid_l,valid_o, o_class, l_class"
   ]
  },
  {
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "**Field**</br> \n",
    "- Defines a datatype together with instructions for converting to Tensor.\n",
    "- It holds a Vocab object that defines the set of possible values for elements of the field and their corresponding numerical representations.\n",
    "- The Field object also holds other parameters relating to how a datatype should be numericalized, such as a tokenization method and the kind of Tensor that should be produced.\n",
    "\n",
    "**Label Field**</br>\n",
    "- A shallow wrapper around a standard field designed to hold labels for a classification task.\n",
    "\n",
    "**Tokenizer**</br>\n",
    "- During processing, _spaCy_ first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language.\n",
    "\n",
    "- ![Tokenizer](https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg)\n",
    "\n",
    "**Bulid Vocab**</br>\n",
    "- Construct the Vocab object for this field from the dataset which occurs with a minimum frequency.\n",
    "- Converts and Stores the numerical representations of the vocabulary using the pre trained weights.\n",
    "\n",
    "**Pretrained Weights: [Glove Embeddings](https://nlp.stanford.edu/projects/glove/)**</br>\n",
    "- Training is performed on aggregated global word-word co-occurrence statistics from a corpus.\n",
    "- The resulting representations showcase interesting linear substructures of the word vector space.\n",
    "    * The Euclidean distance between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.\n",
    "\n",
    "\n",
    "\n",
    "**Bucket Iterator**</br>\n",
    "- Defines an iterator that batches examples of similar lengths together.\n",
    "- Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "import torchtext\n",
    "\n",
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, PATH, batch_size=32):\n",
    "        self.PATH = PATH\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        self.TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=\"spacy\")\n",
    "        self.LABEL = torchtext.legacy.data.LabelField(dtype=torch.long, sequential=False)\n",
    "\n",
    "        self.initData()\n",
    "        self.initEmbed()\n",
    "\n",
    "        self.makeData()\n",
    "\n",
    "    def initData(self):\n",
    "        DATA = os.path.join(self.PATH, 'inputs/')\n",
    "\n",
    "        self.train_data, self.valid_data, self.test_data = torchtext.legacy.data.TabularDataset.splits(\n",
    "                        path=DATA, \n",
    "                        train=\"train.csv\", validation=\"valid.csv\", test=\"test.csv\", \n",
    "                        format=\"csv\", \n",
    "                        skip_header=True, \n",
    "                        fields=[('Text', self.TEXT), ('Label', self.LABEL)])\n",
    "\n",
    "    def initEmbed(self):\n",
    "        EMBED = os.path.join(self.PATH, \"embeddings/glove.840B.300d/glove.840B.300d.txt\")\n",
    "\n",
    "        self.TEXT.build_vocab(self.train_data,\n",
    "                         vectors=torchtext.vocab.Vectors(EMBED), \n",
    "                         max_size=20000, \n",
    "                         min_freq=10)\n",
    "        self.LABEL.build_vocab(self.train_data)\n",
    "\n",
    "    def makeData(self):\n",
    "        self.train_iterator, self.valid_iterator, self.test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "                        (self.train_data, self.valid_data, self.test_data), \n",
    "                        sort_key=lambda x: len(x.Text), \n",
    "                        batch_size=self.batch_size,\n",
    "                        device=self.device)\n",
    "\n",
    "    def lengthData(self):\n",
    "        return len(self.train_data), len(self.valid_data), len(self.test_data)\n",
    "    \n",
    "    def lengthVocab(self):\n",
    "        return len(self.TEXT.vocab), len(self.LABEL.vocab)\n",
    "\n",
    "    def freqLABEL(self):\n",
    "        return self.LABEL.vocab.freqs\n",
    "\n",
    "    def getData(self):\n",
    "        return self.train_iterator, self.valid_iterator, self.test_iterator\n",
    "\n",
    "    def getEmbeddings(self):\n",
    "        return self.TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CreateDataset(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = dataset.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = dataset.getEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset.lengthVocab()[0]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 374\n",
    "output_dim = 2\n",
    "num_layers = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "source": [
    "# Initializing The Model\n",
    "### Long-Short Term Memory Networks\n",
    "Long-Short Term Memory Networks, or __LSTMS__, are a special kind of RNN, capable of learning long-term dependencies. LSTMs are explicitly designed to avoid the long-term dependency problem.\n",
    "\n",
    "**Recurrent Neural Networks**</br>\n",
    "All RNNs have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n",
    "![RNN](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "\n",
    "**[Long-Short Term Memory Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)**</br>\n",
    "LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.\n",
    "![LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "<p align=\"center\">\n",
    "  <img width=\"300\" height=\"200\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7dee414820d5c0162ae1fff1899e58b08923944f\">\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_layers, hidden_dim, static=False, dropout=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, \n",
    "                                         num_layers=num_layers,\n",
    "                                         bidirectional=True, \n",
    "                                         dropout=dropout, \n",
    "                                         batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim*num_layers*2, 1)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = torch.transpose(embedded, dim0=1, dim1=0)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        out = self.linear(self.dropout(torch.cat([cell[i,:, :] for i in range(cell.shape[0])], dim=1)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim, embedding_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data = pretrained_embeddings.to(device)\n",
    "class_weights = torch.tensor([1.0, 15.0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "epoch_val_losses = []\n",
    "accu_train_epoch = []\n",
    "accu_test_epoch = []\n",
    "accu_val_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = torch.round(preds)\n",
    "\n",
    "    correct = (preds == y).float()\n",
    "    acc = correct.sum()/float(len(correct))\n",
    "    return acc"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    train_loss_batch = []\n",
    "    accu_train_batch = []\n",
    "    model.train()\n",
    "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model.forward(batch.Text).view(-1)\n",
    "        batch.Label = (batch.Label).type_as(predictions)\n",
    "        train_loss = criterion(predictions, batch.Label)\n",
    "        acc = binary_accuracy(predictions, batch.Label)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_batch.append(train_loss)\n",
    "        accu_train_batch.append(acc)\n",
    "        bar.update()\n",
    "\n",
    "    epoch_train_losses.append(sum(train_loss_batch)/len(iterator))\n",
    "    accu_train_epoch.append(sum(accu_train_batch)/len(iterator))\n",
    "\n",
    "    return epoch_train_losses[-1], accu_train_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    val_loss_batch = []\n",
    "    accu_val_batch = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model.forward(batch.Text).view(-1)\n",
    "            batch.Label = (batch.Label).type_as(predictions)\n",
    "            val_loss = criterion(predictions, batch.Label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.Label)\n",
    "\n",
    "            val_loss_batch.append(val_loss)\n",
    "            accu_val_batch.append(acc)\n",
    "            bar.update()\n",
    "        epoch_val_losses.append(sum(val_loss_batch)/len(iterator))\n",
    "        accu_val_epoch.append(sum(accu_val_batch)/len(iterator))\n",
    "    return epoch_val_losses[-1], accu_val_epoch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  }
 ]
}