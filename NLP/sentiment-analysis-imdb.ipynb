{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torchtext\nimport spacy\n\nclass CreateDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root_dir, batch_size=32):\n        self.root_dir = root_dir\n        self.batch_size = batch_size\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        self.spacy = spacy.load(\"en_core_web_sm\")\n\n        self.TEXT = torchtext.data.Field(sequential=True, tokenize=\"spacy\")\n        self.LABEL = torchtext.data.LabelField(dtype=torch.long, sequential=False)\n\n        self.initData()\n        self.initEmbed()\n\n        self.makeData()\n\n    def initData(self):\n        \n        df_path = self.root_dir + 'imdb-dataset-sentiment-analysis-in-csv-format'\n\n        self.train_data, self.valid_data, self.test_data = torchtext.data.TabularDataset.splits(\n                        path=df_path, \n                        train=\"Train.csv\", validation=\"Valid.csv\", test=\"Test.csv\", \n                        format=\"csv\", \n                        skip_header=True, \n                        fields=[('Text', self.TEXT), ('Label', self.LABEL)])\n\n    def initEmbed(self):\n        \n        embed_path = self.root_dir + 'glove6b300dtxt/glove.6B.300d.txt'\n\n        self.TEXT.build_vocab(self.train_data,\n                         vectors=torchtext.vocab.Vectors(embed_path), \n                         max_size=20000, \n                         min_freq=10)\n        self.LABEL.build_vocab(self.train_data)\n\n    def makeData(self):\n        self.train_iterator, self.valid_iterator, self.test_iterator = torchtext.data.BucketIterator.splits(\n                        (self.train_data, self.valid_data, self.test_data), \n                        sort_key=lambda x: len(x.Text), \n                        batch_size=self.batch_size,\n                        device=self.device)\n\n    def lengthData(self):\n        return len(self.train_data), len(self.valid_data), len(self.test_data)\n    \n    def lengthVocab(self):\n        return len(self.TEXT.vocab), len(self.LABEL.vocab)\n\n    def freqLABEL(self):\n        return self.LABEL.vocab.freqs\n\n    def getData(self):\n        return self.train_iterator, self.valid_iterator, self.test_iterator\n\n    def getEmbeddings(self):\n        return self.TEXT.vocab.vectors","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:48:07.594369Z","iopub.execute_input":"2021-06-09T13:48:07.594711Z","iopub.status.idle":"2021-06-09T13:48:11.456165Z","shell.execute_reply.started":"2021-06-09T13:48:07.594636Z","shell.execute_reply":"2021-06-09T13:48:11.455297Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndataset = CreateDataset('../input/')\ntrain_iterator, valid_iterator, test_iterator = dataset.getData()\npretrained_embeddings = dataset.getEmbeddings()\npretrained_embeddings.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:48:11.459504Z","iopub.execute_input":"2021-06-09T13:48:11.459797Z","iopub.status.idle":"2021-06-09T13:51:02.096022Z","shell.execute_reply.started":"2021-06-09T13:48:11.459769Z","shell.execute_reply":"2021-06-09T13:51:02.095226Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n100%|█████████▉| 399999/400000 [00:54<00:00, 7362.74it/s]\n/opt/conda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RNN(torch.nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n        \n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\n        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, text):\n        \n        embedded = self.embedding(text)\n        \n        output, hidden = self.rnn(embedded)\n        \n        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n        \n        out = self.linear(hidden)\n        return out\n\nclass LSTM(torch.nn.Module):\n    def __init__(self, input_dim, embedding_dim, num_layers, hidden_dim, dropout = 0.2, bidirectional = False):\n        super(LSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.bidirectional = bidirectional\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n        self.embedding.load_state_dict({'weight': pretrained_embeddings})\n        self.embedding.weight.requires_grad = False\n\n        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, \n                                         num_layers=num_layers,\n                                         bidirectional=bidirectional,\n                                         dropout=dropout, \n                                         batch_first=True)\n        if bidirectional:\n            self.linear = torch.nn.Linear(hidden_dim*num_layers*2, 2)\n        else:\n            self.linear = torch.nn.Linear(hidden_dim*num_layers, 2)\n    def forward(self, text):\n        embedded = self.embedding(text)\n        embedded = torch.transpose(embedded, dim0=1, dim1=0)\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n        out = self.linear(self.dropout(torch.cat([cell[i,:, :] for i in range(cell.shape[0])], dim=1)))\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:02.097855Z","iopub.execute_input":"2021-06-09T13:51:02.098209Z","iopub.status.idle":"2021-06-09T13:51:02.110624Z","shell.execute_reply.started":"2021-06-09T13:51:02.098171Z","shell.execute_reply":"2021-06-09T13:51:02.109577Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\na = torch.tensor([[1,2,3],[4,5,6], [7,8,9]])\nb = torch.tensor([[10,11,12],[13,14,15],[16,17,18]])\nc = torch.cat([a,b], dim = 1)\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:02.112607Z","iopub.execute_input":"2021-06-09T13:51:02.113160Z","iopub.status.idle":"2021-06-09T13:51:02.131316Z","shell.execute_reply.started":"2021-06-09T13:51:02.113123Z","shell.execute_reply":"2021-06-09T13:51:02.130173Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor([[ 1,  2,  3, 10, 11, 12],\n        [ 4,  5,  6, 13, 14, 15],\n        [ 7,  8,  9, 16, 17, 18]])\n","output_type":"stream"}]},{"cell_type":"code","source":"input_dim = dataset.lengthVocab()[0]\nembedding_dim = 300\nhidden_dim = 256\noutput_dim = 2\nnum_layers = 2\nbatch_size = 32\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#model = RNN(input_dim, embedding_dim, hidden_dim, output_dim)\nmodel = LSTM(input_dim, embedding_dim, num_layers, hidden_dim, bidirectional = True)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:02.132862Z","iopub.execute_input":"2021-06-09T13:51:02.133271Z","iopub.status.idle":"2021-06-09T13:51:03.051319Z","shell.execute_reply.started":"2021-06-09T13:51:02.133231Z","shell.execute_reply":"2021-06-09T13:51:03.050562Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LSTM(\n  (dropout): Dropout(p=0.2, inplace=False)\n  (embedding): Embedding(20002, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (linear): Linear(in_features=1024, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\n\noptimizer = optim.SGD(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\nmodel = model.to(device)\ncriterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:03.054174Z","iopub.execute_input":"2021-06-09T13:51:03.054417Z","iopub.status.idle":"2021-06-09T13:51:03.062394Z","shell.execute_reply.started":"2021-06-09T13:51:03.054392Z","shell.execute_reply":"2021-06-09T13:51:03.061591Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef accuracy(preds, y):\n\n    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\n    correct = (ind == y).float()\n    acc = correct.sum()/float(len(correct))\n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:03.064146Z","iopub.execute_input":"2021-06-09T13:51:03.064565Z","iopub.status.idle":"2021-06-09T13:51:03.070687Z","shell.execute_reply.started":"2021-06-09T13:51:03.064516Z","shell.execute_reply":"2021-06-09T13:51:03.069844Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pyprind\n\ndef train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n    for batch in iterator:\n        \n        optimizer.zero_grad()\n                \n        predictions = model(batch.Text).squeeze(0)\n\n        loss = criterion(predictions, batch.Label)\n\n        acc = accuracy(predictions, batch.Label)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        bar.update()\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        #bar = pyprind.ProgBar(len(iterator), bar_char='█')\n        for batch in iterator:\n\n            predictions = model(batch.Text).squeeze(0)\n            \n            loss = criterion(predictions, batch.Label)\n            \n            acc = accuracy(predictions, batch.Label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            #bar.update()\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:03.073169Z","iopub.execute_input":"2021-06-09T13:51:03.073764Z","iopub.status.idle":"2021-06-09T13:51:03.089931Z","shell.execute_reply.started":"2021-06-09T13:51:03.073725Z","shell.execute_reply":"2021-06-09T13:51:03.089028Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"epochs = 20\nbest_acc = 0\nfor epoch in range(epochs):\n\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    if valid_acc > best_acc:\n        torch.save(model.state_dict(), 'weights_lstm_sentiment.pth')\n    print(f'Epoch: {epoch+1} \\t Train Loss: {train_loss:.3f}  \\t Train Acc: {train_acc*100:.2f}% \\nVal. Loss: {valid_loss:.3f} \\t Val. Acc: {valid_acc*100:.2f}% ')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:51:03.091311Z","iopub.execute_input":"2021-06-09T13:51:03.091791Z","iopub.status.idle":"2021-06-09T16:03:12.429207Z","shell.execute_reply.started":"2021-06-09T13:51:03.091675Z","shell.execute_reply":"2021-06-09T16:03:12.428309Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:29\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1 \t Train Loss: 0.689  \t Train Acc: 54.02% \nVal. Loss: 0.686 \t Val. Acc: 53.74% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:30\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2 \t Train Loss: 0.681  \t Train Acc: 57.61% \nVal. Loss: 0.678 \t Val. Acc: 56.55% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:29\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3 \t Train Loss: 0.669  \t Train Acc: 60.03% \nVal. Loss: 0.664 \t Val. Acc: 59.65% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:30\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4 \t Train Loss: 0.647  \t Train Acc: 62.47% \nVal. Loss: 0.634 \t Val. Acc: 64.23% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:29\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5 \t Train Loss: 0.619  \t Train Acc: 65.48% \nVal. Loss: 0.578 \t Val. Acc: 69.53% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:28\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6 \t Train Loss: 0.601  \t Train Acc: 68.09% \nVal. Loss: 0.544 \t Val. Acc: 75.10% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:30\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7 \t Train Loss: 0.657  \t Train Acc: 60.43% \nVal. Loss: 0.686 \t Val. Acc: 53.03% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:31\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8 \t Train Loss: 0.692  \t Train Acc: 53.72% \nVal. Loss: 0.680 \t Val. Acc: 55.06% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:34\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9 \t Train Loss: 0.679  \t Train Acc: 57.06% \nVal. Loss: 0.663 \t Val. Acc: 60.43% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:32\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10 \t Train Loss: 0.660  \t Train Acc: 60.43% \nVal. Loss: 0.669 \t Val. Acc: 59.36% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:31\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11 \t Train Loss: 0.642  \t Train Acc: 63.84% \nVal. Loss: 0.651 \t Val. Acc: 63.95% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:35\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12 \t Train Loss: 0.632  \t Train Acc: 64.80% \nVal. Loss: 0.572 \t Val. Acc: 71.95% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:32\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13 \t Train Loss: 0.612  \t Train Acc: 67.00% \nVal. Loss: 0.536 \t Val. Acc: 73.71% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:33\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14 \t Train Loss: 0.592  \t Train Acc: 69.10% \nVal. Loss: 0.660 \t Val. Acc: 63.38% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:30\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15 \t Train Loss: 0.576  \t Train Acc: 69.80% \nVal. Loss: 0.453 \t Val. Acc: 80.25% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:33\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16 \t Train Loss: 0.461  \t Train Acc: 79.66% \nVal. Loss: 0.420 \t Val. Acc: 81.53% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:33\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17 \t Train Loss: 0.405  \t Train Acc: 82.54% \nVal. Loss: 0.388 \t Val. Acc: 82.76% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:30\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 18 \t Train Loss: 0.373  \t Train Acc: 84.25% \nVal. Loss: 0.345 \t Val. Acc: 85.49% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:35\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19 \t Train Loss: 0.358  \t Train Acc: 84.95% \nVal. Loss: 0.342 \t Val. Acc: 85.67% \n","output_type":"stream"},{"name":"stderr","text":"0% [██████████████████████████████] 100% | ETA: 00:00:00\nTotal time elapsed: 00:06:31\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 20 \t Train Loss: 0.349  \t Train Acc: 85.47% \nVal. Loss: 0.340 \t Val. Acc: 85.51% \n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'Val. Loss: {test_loss:.3f} \\t Val. Acc: {test_acc*100:.2f}% ')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:03:12.430765Z","iopub.execute_input":"2021-06-09T16:03:12.431136Z","iopub.status.idle":"2021-06-09T16:03:17.013896Z","shell.execute_reply.started":"2021-06-09T16:03:12.431098Z","shell.execute_reply":"2021-06-09T16:03:17.013090Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Val. Loss: 0.358 \t Val. Acc: 84.75% \n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('./weights_lstm_sentiment.pth'))\ntrain_loss, train_acc = evaluate(model, train_iterator, criterion)\nprint(f'train. Loss: {train_loss:.3f} \\t train. Acc: {train_acc*100:.2f}% ')\nval_loss, val_acc = evaluate(model, valid_iterator, criterion)\nprint(f'Val. Loss: {val_loss:.3f} \\t Val. Acc: {val_acc*100:.2f}% ')\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\nprint(f'test. Loss: {test_loss:.3f} \\t test. Acc: {test_acc*100:.2f}% ')","metadata":{"execution":{"iopub.status.busy":"2021-06-09T16:03:17.015246Z","iopub.execute_input":"2021-06-09T16:03:17.015585Z","iopub.status.idle":"2021-06-09T16:05:26.498331Z","shell.execute_reply.started":"2021-06-09T16:03:17.015536Z","shell.execute_reply":"2021-06-09T16:05:26.497443Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"train. Loss: 0.353 \t train. Acc: 85.10% \nVal. Loss: 0.340 \t Val. Acc: 85.51% \ntest. Loss: 0.358 \t test. Acc: 84.75% \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}